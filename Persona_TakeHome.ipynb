{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TakeHome assignment for PersonaAI - Prompt Caching"
      ],
      "metadata": {
        "id": "eygAnoDp8Urr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KICLxvNO8Q9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75aAO9y4CoBF",
        "outputId": "25fbfe1d-542b-4fd5-cb4a-7963072a67db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch sentencepiece tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the repository"
      ],
      "metadata": {
        "id": "XuJmblgs8Mmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Guzzler/Takehome-PersonaAI.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPq5uufbDiO3",
        "outputId": "50903cd1-6c98-4b64-eb06-ad1cf3363302"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Takehome-PersonaAI'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 17 (delta 4), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (17/17), 12.47 KiB | 12.47 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download models from LLAMA2 Link\n",
        "\n",
        "You may download any model and change the checkpoint path in the `inference.py` file (By default it is llama2-7b)"
      ],
      "metadata": {
        "id": "yeGs_yiP6cg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash Takehome-PersonaAI/download.sh"
      ],
      "metadata": {
        "id": "6PokwfRZD8y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the benchmarking example for the cached prompt and with one long cache and multiple small examples off that cache.\n",
        "\n",
        "This covers the case of - Preloading ‘instruct’ style prompts for a task containing the overall task instructions but not the input, so you can just append the input later and run inferencing.\n",
        "\n",
        "I have setup an example for the benchmark. One can easily make small changes to inference.py, multiple prompts can be cached using different cache_id identifiers (unique integers that store different states if required)"
      ],
      "metadata": {
        "id": "172uuvSC8-2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Takehome-PersonaAI/inference.py --task system"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfvQx08iHhCw",
        "outputId": "7c697d79-140d-4594-bb0d-dd8f6bed9387"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint \"llama-2-7b/consolidated.00.pth\"\n",
            "Loaded checkpoint in 11.57s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "Loaded state dict in 0.22s\n",
            "Generating tokens: 100% 468/468 [00:15<00:00, 29.77it/s]\n",
            "Generating tokens: 100% 24/24 [00:00<00:00, 30.78it/s]\n",
            "Generating tokens: 100% 24/24 [00:00<00:00, 31.02it/s]\n",
            "This is the generation without the prompt ---> Text: \"I am very angry at her. I want to pull out her hair\". Emotion: Anger\n",
            "\n",
            "Generating tokens: 100% 28/28 [00:00<00:00, 30.88it/s]\n",
            "Generating tokens: 100% 492/492 [00:15<00:00, 31.01it/s]\n",
            "Generating tokens: 100% 496/496 [00:16<00:00, 30.99it/s]\n",
            "Cache setup time: 15.76861310005188s\n",
            "Completion time with cache (Prompt 1): 0.7743737697601318s, Response: Text: \"I am very angry at her. I want to pull out her hair\". Emotion: Anger\n",
            "\n",
            "Completion time with cache (Prompt 2): 0.9078752994537354s, Response: Text: \"Everytime I think about her it makes me want to cry. I am heartbroken\". Emotion: \"Sad\n",
            "Completion time without cache (Prompt 1): 15.868504047393799s, Response: Task Description:\n",
            "          You are an advanced language model trained to understand and classify emotions conveyed in text. Your task is to read short passages and identify the primary emotion expressed from the following options: joy, sadness, anger, fear, surprise, disgust, trust, and anticipation. For each passage, choose the emotion that best matches the overall sentiment of the text.\n",
            "\n",
            "          Few-Shot Examples:\n",
            "          Text: \"Winning the championship after months of hard work felt incredibly rewarding. The team's joy was palpable, and celebrations lasted throughout the night.\"\n",
            "          Emotion: Joy\n",
            "\n",
            "          Text: \"The news of his grandmother's passing hit him hard. He found himself reminiscing about the summer vacations spent at her house, feeling a deep sense of loss.\"\n",
            "          Emotion: Sadness\n",
            "\n",
            "          Text: \"She couldn't believe her colleague blatantly lied about her in front of the boss. The injustice of the situation filled her with anger.\"\n",
            "          Emotion: Anger\n",
            "\n",
            "          Text: \"Walking home alone, he heard footsteps echoing behind him. Turning around and seeing no one, fear gripped him as he quickened his pace.\"\n",
            "          Emotion: Fear\n",
            "\n",
            "          Text: \"The surprise party for her 30th birthday left her speechless. She had no idea her friends and family could pull off something so elaborate without her finding out.\"\n",
            "          Emotion: Surprise\n",
            "\n",
            "          Text: \"Finding the rotten vegetables in the fridge, forgotten and hidden behind the milk carton, made her recoil in disgust.\"\n",
            "          Emotion: Disgust\n",
            "\n",
            "          Text: \"After years of working together, they had built a foundation of trust. He knew he could count on his team to support him, no matter the challenge ahead.\"\n",
            "          Emotion: Trust\n",
            "\n",
            "          Text: \"The night before the launch, she was filled with anticipation. All their planning and hard work were about to be put to the test.\"\n",
            "          Emotion: Anticipation\n",
            "\n",
            "          Text: \"I am very angry at her. I want to pull out her hair\". Emotion: Anger\n",
            "\n",
            "Completion time without cache (Prompt 2): 16.007229566574097s, Response: Task Description:\n",
            "          You are an advanced language model trained to understand and classify emotions conveyed in text. Your task is to read short passages and identify the primary emotion expressed from the following options: joy, sadness, anger, fear, surprise, disgust, trust, and anticipation. For each passage, choose the emotion that best matches the overall sentiment of the text.\n",
            "\n",
            "          Few-Shot Examples:\n",
            "          Text: \"Winning the championship after months of hard work felt incredibly rewarding. The team's joy was palpable, and celebrations lasted throughout the night.\"\n",
            "          Emotion: Joy\n",
            "\n",
            "          Text: \"The news of his grandmother's passing hit him hard. He found himself reminiscing about the summer vacations spent at her house, feeling a deep sense of loss.\"\n",
            "          Emotion: Sadness\n",
            "\n",
            "          Text: \"She couldn't believe her colleague blatantly lied about her in front of the boss. The injustice of the situation filled her with anger.\"\n",
            "          Emotion: Anger\n",
            "\n",
            "          Text: \"Walking home alone, he heard footsteps echoing behind him. Turning around and seeing no one, fear gripped him as he quickened his pace.\"\n",
            "          Emotion: Fear\n",
            "\n",
            "          Text: \"The surprise party for her 30th birthday left her speechless. She had no idea her friends and family could pull off something so elaborate without her finding out.\"\n",
            "          Emotion: Surprise\n",
            "\n",
            "          Text: \"Finding the rotten vegetables in the fridge, forgotten and hidden behind the milk carton, made her recoil in disgust.\"\n",
            "          Emotion: Disgust\n",
            "\n",
            "          Text: \"After years of working together, they had built a foundation of trust. He knew he could count on his team to support him, no matter the challenge ahead.\"\n",
            "          Emotion: Trust\n",
            "\n",
            "          Text: \"The night before the launch, she was filled with anticipation. All their planning and hard work were about to be put to the test.\"\n",
            "          Emotion: Anticipation\n",
            "\n",
            "          Text: \"Everytime I think about her it makes me want to cry. I am heartbroken\". Emotion: Sadness\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the benchmarking example for the chat type structure\n",
        "\n",
        "This covers the case of - Typical chat/assistant style where new lines are added to the prompt (so you will want to save after every inference)\n",
        "\n",
        "I have setup an example for the benchmark. One can easily make small changes to inference.py, multiple chats can be cached using different cache_id identifiers (unique integers that store different states of chats if required)\n",
        "\n"
      ],
      "metadata": {
        "id": "0MTXGarF9tkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ppcJkcjQArbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Takehome-PersonaAI/inference.py --task chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z6UxLj5yrw6",
        "outputId": "783421b9-3b79-4347-91e6-beabf256c91a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint \"llama-2-7b/consolidated.00.pth\"\n",
            "Loaded checkpoint in 11.54s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "Loaded state dict in 0.22s\n",
            "Generating tokens: 100% 63/63 [00:02<00:00, 23.06it/s]\n",
            "Generating tokens: 100% 12/12 [00:00<00:00, 30.98it/s]\n",
            "Generating tokens: 100% 12/12 [00:00<00:00, 30.92it/s]\n",
            "Generating tokens: 100% 74/74 [00:02<00:00, 31.00it/s]\n",
            "Generating tokens: 100% 85/85 [00:02<00:00, 31.02it/s]\n",
            "Initial Chat time: 2.77986741065979s\n",
            "Completion time with cache (Chat 2): 0.3882172107696533s, Response: \n",
            "      21\n",
            "      \n",
            "      34\n",
            "\n",
            "Completion time with cache (Chat 3): 0.38900065422058105s, Response: \n",
            "      55\n",
            "      \n",
            "      89\n",
            "\n",
            "Completion time without cache (Chat 1+2): 2.3885085582733154s, Response: Let's play a game with numbers. For each number add the previous two generated number to this and return it\n",
            "      Examples:\n",
            "      1\n",
            "      \n",
            "      1\n",
            "      \n",
            "      2\n",
            "      \n",
            "      3\n",
            "      \n",
            "      5\n",
            "      \n",
            "      8\n",
            "      \n",
            "      \n",
            "13\n",
            "      21\n",
            "      \n",
            "      34\n",
            "\n",
            "Completion time without cache (Chat 1+2+3): 2.741135358810425s, Response: Let's play a game with numbers. For each number add the previous two generated number to this and return it\n",
            "      Examples:\n",
            "      1\n",
            "      \n",
            "      1\n",
            "      \n",
            "      2\n",
            "      \n",
            "      3\n",
            "      \n",
            "      5\n",
            "      \n",
            "      8\n",
            "      \n",
            "      \n",
            "13\n",
            "      21\n",
            "      \n",
            "      \n",
            "34\n",
            "      55\n",
            "      \n",
            "      \n",
            "89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future work\n",
        "\n",
        "There is specific feature where after generation a state of the chat can be restored to the previous_state(s) (This was added as an extra functionality for things like regenerate in ChatGPT) but was not implemented due to the overhead (dictionary of dictionaries) and extra storage required.\n",
        "\n",
        "Implementation of more modular caching for low-latencies used in this paper - https://arxiv.org/abs/2311.04934 (Prompt Cache)"
      ],
      "metadata": {
        "id": "i5pjDE4AAsHL"
      }
    }
  ]
}
